{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import urllib.request as ur\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "第1页完成\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "第2页完成\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "第3页完成\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "第4页完成\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "第5页完成\n",
      "1\n",
      "第6页完成\n",
      "第7页完成\n",
      "第8页完成\n",
      "第9页完成\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for k in range(1,6):\n",
    "    # 根据网址获取地址\n",
    "    req = ur.Request('http://cs.fang.lianjia.com/loupan/pg'+str(k))\n",
    "    \n",
    "    # 读取网页\n",
    "    response = ur.urlopen(req)\n",
    "    the_page = response.read()\n",
    "    \n",
    "    #解析网页\n",
    "    soup = BeautifulSoup(the_page, \"lxml\")\n",
    "    list0 = []\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "    list4 = []\n",
    "    list5 = []\n",
    "    list6 = []\n",
    "    \n",
    "    #提取楼盘名称字段\n",
    "    for tag in soup.find_all(name = 'div', attrs = {\"class\": re.compile(\"col-1\")}):\n",
    "        ta1 = tag.find(name = \"a\", attrs = {\"target\": re.compile(\"_blank\")})\n",
    "        # 添加城市字段\n",
    "        list0.append('长沙')\n",
    "        list1.append(ta1.string)\n",
    "        \n",
    "        # 提取建筑面积字段\n",
    "        ta2 = tag.find(name = 'div', attrs = {'class': re.compile('area')})\n",
    "        t2 = ta2.find(name = 'span')\n",
    "        if t2 != None:\n",
    "            list2.append(t2.string)\n",
    "        else:\n",
    "            list2.append(0)\n",
    "        \n",
    "        # 提取在售状态字段\n",
    "        ta3 = tag.find(name = 'span', attrs = {'class': re.compile('onsold')})\n",
    "        list3.append(ta3.string)\n",
    "        \n",
    "        \n",
    "        # 提取住宅类型字段\n",
    "        ta4 = tag.find(name = \"span\", attrs = {'class': re.compile('live')})\n",
    "        list4.append(ta4.string)\n",
    "        \n",
    "        # 提取每平米均价字段\n",
    "    for tag in soup.find_all(name = 'div', attrs = {'class': re.compile('col-2')}):\n",
    "        ta5 = tag.find(name = 'span', attrs = {'class': re.compile('num')})\n",
    "        if ta5 != None:\n",
    "            list5.append(ta5.string)\n",
    "        else:\n",
    "            list5.append(0)\n",
    "        # 提取总价字段\n",
    "        ta6 = tag.find(name = 'div', attrs = {'class': re.compile('sum-num')})\n",
    "        if ta6 != None:\n",
    "            t6 = ta6.find(name = 'span')\n",
    "            list6.append(t6.string)\n",
    "        else:\n",
    "            list6.append(0)\n",
    "            print(1)\n",
    "    # 将提取的数据合并\n",
    "    \n",
    "    for i in range(0,len(soup.find_all(name = 'div', attrs = {'class': re.compile('col-1')}))):\n",
    "        data.append((list0[i], list1[i], list2[i], list3[i], list4[i], list5[i], list6[i]))\n",
    "        \n",
    "    print('第{}页完成'.format(str(k)))\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_csv('lj.csv', encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city = pd.read_csv('ljcity.csv',encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(city)):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
